---
title: "Machine learning models for cancer predictive analysis"
author: "Natalia"
date: "28 May 2019"
output: pdf_document
---
```{r}

data <- read.table("C://Users//Natalia//Desktop//ITMO//R//R project//cancer data//lympha//lymphography.data", header = TRUE, stringsAsFactors = TRUE, sep = ",")
colnames(data) <- c("class", "lymphatics", "blaff", "blc", "bls", "pass", "extravasats", "regeneration", "uptake", "nodesdim", "nodesenlar", "chlym", "defect", "chnode", "chstru", "forms", "disloc", "exclusion", "numbnodes")
View(data)
```

#Analyse the dataset and tidy it up.
```{r}
# Analyse the data - checking for values, NAs, data type.
summary(data)
str(data)
```

```{r}
#Comments for dataset:

writeLines(readLines("C://Users//Natalia//Desktop//ITMO//R//R project//cancer data//lympha//lymph.txt"))
```


```{r}
#Replace class "numeric" specification for strings abbreviation:

data$Class <- ifelse(data$class== 1, "normal", ifelse(data$class == 2, "metastases", ifelse(data$class == 3, "malign", "fibrosis")))
data$class <- NULL
head(data)
dim(data)

```

```{r}
library(tidyverse)
map_int(data, function(.x) sum(is.na(.x)))
```


```{r}
# Data type "Class" as factor:

data <- as.data.frame(data, stringsAsFactors=T)
data$Class <- as.factor(data$Class)
head(data)
```


#DATA EXPLORATION

#Hierarchical clustering

```{r}

library(sparcl)
hc <- hclust(dist(data[,1:18]), method = "complete")
ColorDendrogram(hc,y=data$Class, main = "Hierarchical clustering", branchlength=5)

```

Most of the "green class"" and "red class"" samples are grouped together with two prevalent clusters. 

#K-means clustering
```{r}
fit <- kmeans(data[,c(1:18)], 2)
names(fit)
```

```{r}
#k-means did a fairly good job
table(data.frame(fit$cluster,data[,19]))

```

#Response variable for classification

```{r}
library(ggplot2)

ggplot(data, aes(x = Class, fill = Class)) +
  geom_bar()
```

#Response variable for regression

```{r}
ggplot(data, aes(x = lymphatics)) +
  geom_histogram(binwidth = 1, stat = "count")
```

#Principal Component Analysis

```{r}
library(pcaGoPromoter)
library(ellipse)
```

```{r}
data <-na.omit(data)
```


```{r}
# perform pca and extract scores:

pcaOutput <- pca(t(data[,1:18]), printDropped = FALSE, scale = TRUE, center = TRUE)
pcaOutput2 <- as.data.frame(pcaOutput$scores)
```


```{r}
# define groups for plotting:

pcaOutput2$groups <- data$Class

centroids <- aggregate(cbind(PC1, PC2) ~ groups, pcaOutput2, mean)

conf.rgn  <- do.call(rbind, lapply(unique(pcaOutput2$groups), function(t)
  data.frame(groups = as.character(t),
             ellipse(cov(pcaOutput2[pcaOutput2$groups == t, 1:2]),
                   centre = as.matrix(centroids[centroids$groups == t, 2:3]),
                   level = 0.95),
             stringsAsFactors = FALSE)))


#Plot PCA with variance %:

ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, color = groups)) + 
    geom_polygon(data = conf.rgn, aes(fill = groups), alpha = 0.2) +
    geom_point(size = 2, alpha = 0.6) + 
    scale_color_brewer(palette = "Set1") +
    labs(color = "",
         fill = "",
         x = paste0("PC1: ", round(pcaOutput$pov[1], digits = 2) * 100, "% variance"),
         y = paste0("PC2: ", round(pcaOutput$pov[2], digits = 2) * 100, "% variance")) 
```

Features

```{r fig.height = 12, fig.width = 7}
library(tidyr)

gather(data, x, y, lymphatics:numbnodes) %>%
  ggplot(aes(x = y, color = Class, fill = Class)) +
    geom_density(alpha = 0.3) +
    facet_wrap( ~ x, scales = "free", ncol = 3)
```

#MACHINE LEARNING PACKAGES FOR R

#caret

```{r}
# configure multicore
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)

library(caret)
```

#Training, validation and test data

```{r}
set.seed(42)
index <- createDataPartition(data$Class, p = 0.7, list = FALSE)
train_data <- data[index, ]
test_data  <- data[-index, ]
```

```{r fig.height = 12, fig.width = 7}
library(dplyr)

rbind(data.frame(group = "train", train_data),
      data.frame(group = "test", test_data)) %>%
  gather(x, y, lymphatics:numbnodes) %>%
  ggplot(aes(x = y, color = group, fill = group)) +
    geom_density(alpha = 0.3) +
    facet_wrap( ~ x, scales = "free", ncol = 3)
```

#Regression

```{r}
set.seed(42)
model_glm <- caret::train(lymphatics ~ .,
                          data = train_data,
                          method = "glm",
                          preProcess = c("scale", "center"),
                          trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))
```

```{r}
model_glm
```

```{r}
predictions <- predict(model_glm, test_data)
```

```{r}
# model_glm$finalModel$linear.predictors == model_glm$finalModel$fitted.values
data.frame(residuals = resid(model_glm),
           predictors = model_glm$finalModel$linear.predictors) %>%
  ggplot(aes(x = predictors, y = residuals)) +
    geom_jitter() +
    geom_smooth(method = "lm")
```

```{r}
y <- train_data$lymphatics
data.frame(residuals = resid(model_glm),
           y = model_glm$finalModel$y) %>%
  ggplot(aes(x = y, y = residuals)) +
    geom_jitter() +
    geom_smooth(method = "lm")
```

```{r}
data.frame(actual = test_data$lymphatics,
           predicted = predictions) %>%
  ggplot(aes(x = actual, y = predicted)) +
    geom_jitter() +
    geom_smooth(method = "lm")
```

# CLASSIFICATION

# Decision trees

```{r fig.height = 7, fig.width = 12}
library(rpart)
library(rpart.plot)

set.seed(42)
fit <- rpart(Class ~ .,
            data = train_data,
            method = "class",
            control = rpart.control(xval = 10, 
                                    minbucket = 2, 
                                    cp = 0), 
             parms = list(split = "information"))

rpart.plot(fit, extra = 100)

```

# RANDOM FORESTS

```{r}
#Random Forests predictions are based on the generation of
#multiple classification trees.
#They can be used for both, classification and regression tasks. 
#Here, it is classification task.
```

```{r}
set.seed(42)
library(randomForest)
model_rf <- caret::train(Class ~ .,
                         data = train_data,
                         method = "rf",
                         preProcess = c("scale", "center"),
                         trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))
```

```{r}
#When savePredictions = TRUE is specified,
#can access the cross-validation resuls with model_rf$pred.

model_rf$finalModel$confusion
```

#Feature Importance

```{r}
imp <- model_rf$finalModel$importance
imp[order(imp, decreasing = TRUE), ]
```

```{r}
# estimate variable importance
importance <- varImp(model_rf, scale = TRUE)
plot(importance)
```

#Predicting test data

```{r}
confusionMatrix(predict(model_rf, test_data), test_data$Class)
```

```{r}
results <- data.frame(actual = test_data$Class,
                      predict(model_rf, test_data, type = "prob"))

results$prediction <- ifelse(results$metastases > 0.3, "metastases",
                             ifelse(results$malign > 0.3, "malignant", 
                                    ifelse(results$fibrosis > 0.3, "fibrosis", NA)))

results$correct <- ifelse(results$actual == results$prediction, TRUE, FALSE)

ggplot(results, aes(x = prediction, fill = correct)) +
  geom_bar(position = "dodge")
```

```{r}
ggplot(results, aes(x = prediction, y = malign, color = correct, shape = correct)) +
  geom_jitter(size = 3, alpha = 0.6)

```

#EXTREME GRADIENT BOOSTING.

Extreme gradient boosting (XGBoost) is a faster and
improved implementation of gradient boosting for supervised learning.
```{r}
#XGBoost is a tree ensemble model, which means the sum of predictions 
#from a set of classification and regression trees (CART). 
#In that, XGBoost is similar to Random Forests but it uses a different approach 
#to model training: it uses a combination of "weak" functions during iteration process, 
#for each next iteration step, the model learns using the "mistakes" data of previous steps. 
```

```{r}
set.seed(42)
library(xgboost)
model_xgb <- caret::train(Class ~ .,
                          data = train_data,
                          method = "xgbTree",
                          preProcess = c("scale", "center"),
                          trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))
```
#Feature Importance
```{r}
importance <- varImp(model_xgb, scale = TRUE)
plot(importance)
```
#Predicting test data
```{r}
confusionMatrix(predict(model_xgb, test_data), test_data$Class)
```

```{r}
results <- data.frame(actual = test_data$Class,
                      predict(model_xgb, test_data, type = "prob"))

results$prediction <- ifelse(results$metastases > 0.3, "metastases",
                             ifelse(results$malign > 0.3, "malignant", 
                                    ifelse(results$fibrosis > 0.3, "fibrosis", NA)))

results$correct <- ifelse(results$actual == results$prediction, TRUE, FALSE)

ggplot(results, aes(x = prediction, fill = correct)) +
  geom_bar(position = "dodge")
```

```{r}
ggplot(results, aes(x = prediction, y = malign, color = correct, shape = correct)) +
  geom_jitter(size = 3, alpha = 0.6)
```
#FEATURE SELECTION

Performing feature selection on the whole dataset would lead to prediction bias, 
we therefore need to run the whole modeling process on the training data alone!

#Correlation

```{r}
library(corrplot)

# calculate correlation matrix
corMatMy <- cor(train_data[,1:18])
corrplot(corMatMy, order = "hclust")
```

```{r}
#Apply correlation filter at 0.70:
highlyCor <- colnames(train_data[, -1])[findCorrelation(corMatMy, cutoff = 0.6, verbose = TRUE)]
```


```{r}
# which variables are flagged for removal?
highlyCor
#then we remove these variables
train_data_cor <- train_data[, which(!colnames(train_data) %in% highlyCor)]
```
#GRID SEARCH WITH CARET

#Automatic Grid
```{r}
set.seed(42)
model_rf_tune_auto <- caret::train(Class ~ .,
                         data = train_data,
                         method = "rf",
                         preProcess = c("scale", "center"),
                         trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE,
                                                  search = "random"),
                         tuneLength = 15)
```

```{r}
model_rf_tune_auto
```

```{r}
plot(model_rf_tune_auto)
```


#NEURAL NETWORK MODEL
```{r}
library(nnet)
model_nnet<-nnet(Class ~. ,
                 data= train_data,
                 size=8
)
```

```{r}
library(NeuralNetTools)
# Plot a neural interpretation diagram for a neural network object
plotnet(model_nnet, cex_val =.8,max_sp=T,circle_cex=5,circle_col = 'red')
```

```{r}
#Relative importance of input variables in neural networks using Garson's algorithm
#garson(model_nnet)
```

```{r}

olden(model_nnet) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
```
Here both the positve and negative value represents relative 
contibutions of each connection weight among the variables
```{r}
#Predict
predict_nnet <- predict(model_nnet,test_data, type = "class")
```

```{r}
#Draw the crosstable

library(gmodels)
CrossTable(test_data$Class,predict_nnet,prop.chisq = F,prop.r = F,prop.c = F,dnn =c("Actual Diagnosis","Predict Diagnosis"))
```

