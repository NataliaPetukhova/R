---
title: "Machine learning models for cancer predictive analysis"
author: "Natalia"
date: "28 May 2019"
output: 
  pdf_document: 
    fig_caption: yes
    fig_height: 5
    fig_width: 7
---
```{r}

data <- read.csv("C://Users//Natalia//Desktop//ITMO//R//R project//cancer data//breastcancer//cancer_data.csv", header = TRUE, stringsAsFactors = TRUE, sep = ",")
colnames(data) <- c("id", "Class", "radius_mean", "texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave_points_mean", "symmetry_mean", "fractal_dimension_mean", "radius_se", "texture_se", "perimeter_se", "area_se", "smoothness_se", "compactness_se", "concavity_se", "concave_points_se", "symmetry_se", "fractal_dimension_se", "radius_worst", "texture_worst","perimeter_worst","area_worst","smoothness_worst",
                    "compactness_wors", "concavity_worst", "concave points_worst","symmetry_worst","fractal_dimension_worst")
View(data)
```

#Analyse the dataset and tidy it up.
```{r}
# Analyse the data - checking for values, NAs, data type.
summary(data)
str(data)
```

```{r}
data$id <- NULL
data$Cl <- ifelse(data$Class == "0", "benign", ifelse(data$Class == 1, "malignant", NA))
data$Class <- data$Cl
head(data)
dim(data)
```

```{r}
library(tidyverse)
map_int(data, function(.x) sum(is.na(.x)))
```


```{r}
# Data type "Class" as factor:

data <- as.data.frame(data, stringsAsFactors=T)
data$Class <- as.factor(data$Class)
data$Cl <- NULL
head(data)
```


#DATA EXPLORATION

#Hierarchical clustering

```{r}

library(sparcl)
hc <- hclust(dist(data[,-1]), method = "complete")
ColorDendrogram(hc,y=data$Class, main = "Hierarchical clustering", branchlength=5)

```

Not very obvious where are different clusters. It seems that "red" malignant cluster intersects with "benign" cluster.

#K-means clustering
```{r}
fit <- kmeans(data[,c(2:31)], 2)
names(fit)
```

```{r}
#k-means did a fairly good job
table(data.frame(fit$cluster,data[,1]))

```

#Response variable for classification.

```{r}
library(ggplot2)

ggplot(data, aes(x = Class, fill = Class)) +
  geom_bar()
```

#Response variable for regression.

```{r}
ggplot(data, aes(x = radius_mean)) +
  geom_histogram(stat = "count", color = "red")
```

#Principal Component Analysis

```{r}
library(pcaGoPromoter)
library(ellipse)
```

```{r}
data <-na.omit(data)
```


```{r}
# perform pca and extract scores:

pcaOutput <- pca(t(data[,2:31]), printDropped = FALSE, scale = TRUE, center = TRUE)
pcaOutput2 <- as.data.frame(pcaOutput$scores)
```


```{r}
# define groups for plotting:

pcaOutput2$groups <- data$Class

centroids <- aggregate(cbind(PC1, PC2) ~ groups, pcaOutput2, mean)

conf.rgn  <- do.call(rbind, lapply(unique(pcaOutput2$groups), function(t)
  data.frame(groups = as.character(t),
             ellipse(cov(pcaOutput2[pcaOutput2$groups == t, 1:2]),
                   centre = as.matrix(centroids[centroids$groups == t, 2:3]),
                   level = 0.95),
             stringsAsFactors = FALSE)))


#Plot PCA with variance %:

ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, color = groups)) + 
    geom_polygon(data = conf.rgn, aes(fill = groups), alpha = 0.2) +
    geom_point(size = 2, alpha = 0.6) + 
    scale_color_brewer(palette = "Set1") +
    labs(color = "",
         fill = "",
         x = paste0("PC1: ", round(pcaOutput$pov[1], digits = 2) * 100, "% variance"),
         y = paste0("PC2: ", round(pcaOutput$pov[2], digits = 2) * 100, "% variance")) 
```

#Features

```{r fig.height = 15, fig.width = 10}
library(tidyr)


gather(data, x, y, radius_mean:fractal_dimension_worst) %>%
  ggplot(aes(x = y, color = Class, fill = Class)) +
    geom_density(alpha = 0.3) +
    facet_wrap( ~ x, scales = "free", ncol = 3)

```

#MACHINE LEARNING PACKAGES FOR R

#caret
```{r}
# configure multicore:
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)

library(caret)
```

#Training, validation and test data

```{r}
set.seed(42)
index <- createDataPartition(data$Class, p = 0.7, list = FALSE)
train_data <- data[index, ]
test_data  <- data[-index, ]
```

```{r fig.height = 15, fig.width = 10}
library(dplyr)

rbind(data.frame(group = "train", train_data),
      data.frame(group = "test", test_data)) %>%
  gather(x, y, radius_mean:fractal_dimension_worst) %>%
  ggplot(aes(x = y, color = group, fill = group)) +
    geom_density(alpha = 0.3) +
    facet_wrap( ~ x, scales = "free", ncol = 3)
```

#REGRESSION

```{r}
set.seed(42)
model_glm <- caret::train(radius_mean ~ .,
                          data = train_data,
                          method = "glm",
                          preProcess = c("scale", "center"),
                          trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))
```

```{r}
model_glm
```

```{r}
predictions <- predict(model_glm, test_data)
```

```{r}
# model_glm$finalModel$linear.predictors == model_glm$finalModel$fitted.values
data.frame(residuals = resid(model_glm),
           predictors = model_glm$finalModel$linear.predictors) %>%
  ggplot(aes(x = predictors, y = residuals)) +
    geom_jitter() +
    geom_smooth(method = "lm")
```

```{r}
y <- train_data$radius_mean
data.frame(residuals = resid(model_glm),
           y = model_glm$finalModel$y) %>%
  ggplot(aes(x = y, y = residuals)) +
    geom_jitter() +
    geom_smooth(method = "lm")
```

```{r}
data.frame(actual = test_data$radius_mean,
           predicted = predictions) %>%
  ggplot(aes(x = actual, y = predicted)) +
    geom_jitter() +
    geom_smooth(method = "lm")
```

# CLASSIFICATION

# Decision trees

```{r}
library(rpart)
library(rpart.plot)

set.seed(42)
fit <- rpart(Class ~ .,
            data = train_data,
            method = "class",
            control = rpart.control(xval = 10, 
                                    minbucket = 2, 
                                    cp = 0), 
             parms = list(split = "information"))

rpart.plot(fit, extra = 100)

```

# RANDOM FORESTS

```{r}
#Random Forests predictions are based on the generation of
#multiple classification trees.
#They can be used for both, classification and regression tasks. 
#Here, it is classification task.
```

```{r}
set.seed(42)
library(randomForest)
model_rf <- caret::train(Class ~ .,
                         data = train_data,
                         method = "rf",
                         preProcess = c("scale", "center"),
                         trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))
```

```{r}
#When savePredictions = TRUE is specified,
#can access the cross-validation resuls with model_rf$pred.

model_rf$finalModel$confusion
```

#Feature Importance

```{r}
imp <- model_rf$finalModel$importance
imp[order(imp, decreasing = TRUE), ]
```

```{r}
# estimate variable importance
importance <- varImp(model_rf, scale = TRUE)
plot(importance)
```

#Predicting test data

```{r}
confusionMatrix(predict(model_rf, test_data), test_data$Class)
```

```{r}
results <- data.frame(actual = test_data$Class,
                      predict(model_rf, test_data, type = "prob"))

results$prediction <- ifelse(results$benign > 0.5, "benign",
                             ifelse(results$malignant > 0.5, "malignant", NA))

results$correct <- ifelse(results$actual == results$prediction, TRUE, FALSE)

ggplot(results, aes(x = prediction, fill = correct)) +
  geom_bar(position = "dodge")
```

```{r}
ggplot(results, aes(x = prediction, y = malignant, color = correct, shape = correct)) +
  geom_jitter(size = 3, alpha = 0.6)

```

#EXTREME GRADIENT BOOSTING.

Extreme gradient boosting (XGBoost) is a faster and
improved implementation of gradient boosting for supervised learning.
```{r}
#XGBoost is a tree ensemble model, which means the sum of predictions 
#from a set of classification and regression trees (CART). 
#In that, XGBoost is similar to Random Forests but it uses a different approach 
#to model training: it uses a combination of "weak" functions during iteration process, 
#for each next iteration step, the model learns using the "mistakes" data of previous steps. 
```

```{r}
set.seed(42)
library(xgboost)
model_xgb <- caret::train(Class ~ .,
                          data = train_data,
                          method = "xgbTree",
                          preProcess = c("scale", "center"),
                          trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))
```

#Feature Importance
```{r}
importance <- varImp(model_xgb, scale = TRUE)
plot(importance)
```

#Predicting test data
```{r}
confusionMatrix(predict(model_xgb, test_data), test_data$Class)
```

```{r}
results <- data.frame(actual = test_data$Class,
                      predict(model_xgb, test_data, type = "prob"))

results$prediction <- ifelse(results$benign > 0.5, "benign",
                             ifelse(results$malignant > 0.5, "malignant", NA))

results$correct <- ifelse(results$actual == results$prediction, TRUE, FALSE)

ggplot(results, aes(x = prediction, fill = correct)) +
  geom_bar(position = "dodge")
```

```{r}
ggplot(results, aes(x = prediction, y = malignant, color = correct, shape = correct)) +
  geom_jitter(size = 3, alpha = 0.6)
```

#FEATURE SELECTION

Performing feature selection on the whole dataset would lead to prediction bias, 
we therefore need to run the whole modeling process on the training data alone!

#Correlation

```{r}
library(corrplot)

# calculate correlation matrix
corMatMy <- cor(train_data[,2:31])
corrplot(corMatMy, order = "hclust")

#Correlations between all features are calculated and visualised.
#Removing all features with a correlation higher than 0.7, 
#keeping the feature with the lower mean.
```

```{r}
#Apply correlation filter at 0.70:

highlyCor <- colnames(train_data[, -1])[findCorrelation(corMatMy, cutoff = 0.7, verbose = TRUE)]
```

```{r}
# which variables are flagged for removal?
highlyCor
#then we remove these variables
train_data_cor <- train_data[, which(!colnames(train_data) %in% highlyCor)]
```

#GRID SEARCH WITH CARET

#Automatic Grid
```{r}
set.seed(42)
model_rf_tune_auto <- caret::train(Class ~ .,
                         data = train_data,
                         method = "rf",
                         preProcess = c("scale", "center"),
                         trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE,
                                                  search = "random"),
                         tuneLength = 15)
```

```{r}
model_rf_tune_auto
```

```{r}
plot(model_rf_tune_auto)
```

#NEURAL NETWORK MODEL

```{r}

library(nnet)
model_nnet <- nnet(Class ~. ,
                 data= train_data,
                 size=8)
```


```{r}
library(NeuralNetTools)
# Plot a neural interpretation diagram for a neural network object
plotnet(model_nnet, cex_val =.8,max_sp=T,circle_cex=5,circle_col = 'red')
```

```{r}
#Relative importance of input variables in neural networks using Garson's algorithm:
garson(model_nnet, las = 2) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}

olden(model_nnet) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
Here both the positve and negative value represents relative 
contibutions of each connection weight among the variables

#Prediction
```{r}
#Predict
predict_nnet <- predict(model_nnet,test_data, type = "class")
```

```{r}
#Draw the crosstable

library(gmodels)
CrossTable(test_data$Class,predict_nnet,prop.chisq = F,prop.r = F,prop.c = F,dnn =c("Actual Diagnosis","Predict Diagnosis"))
```

